---
title: CompTIA Data+ Domain
categories:
- data
excerpt: |
## CompTIA Data+ Domain excerpt
feature_text: |  
  ## CompTIA Data+ Domain features
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
aside: true
---

- [Domain 1.0 Data Concepts and Environments](#domain-10-data-concepts-and-environments)
  - [1.1 Identitfy basic concepts of data schemas and dimensions](#11-identitfy-basic-concepts-of-data-schemas-and-dimensions)
  - [1.2 Compare and contrast different data types](#12-compare-and-contrast-different-data-types)
  - [1.3 Compare and contrast common data structures and file formats](#13-compare-and-contrast-common-data-structures-and-file-formats)
- [Domain 2.0 Data Mining](#domain-20-data-mining)
  - [2.1 Explain data acquisition concepts](#21-explain-data-acquisition-concepts)
  - [2.2 Identify common reason for cleansing and profiling datasets](#22-identify-common-reason-for-cleansing-and-profiling-datasets)
  - [2.3 Given a scenario, execute data manipulation techniques](#23-given-a-scenario-execute-data-manipulation-techniques)
  - [2.4 Explain common techniques for data manipulation and query optimization](#24-explain-common-techniques-for-data-manipulation-and-query-optimization)
- [Domain 3.0 Data Analysis](#domain-30-data-analysis)
  - [3.1 Given a scenario, apply the appropriate descriptive statistical methods](#31-given-a-scenario-apply-the-appropriate-descriptive-statistical-methods)
  - [3.2 Explain the purpose of inferential statistical methods](#32-explain-the-purpose-of-inferential-statistical-methods)
  - [3.3 Summarize types of analysis and key analysis techniques](#33-summarize-types-of-analysis-and-key-analysis-techniques)
  - [3.4 Identify common data analytics tools](#34-identify-common-data-analytics-tools)
- [Domain 4.0 Visualization](#domain-40-visualization)
  - [4.1 Given a scenario, translate business requirements to form a report](#41-given-a-scenario-translate-business-requirements-to-form-a-report)
  - [4.2 Given a scenario, use appropriate design components for reports and dashboards](#42-given-a-scenario-use-appropriate-design-components-for-reports-and-dashboards)
  - [4.3 Given a scenario, use appropriate methods for dashboards development](#43-given-a-scenario-use-appropriate-methods-for-dashboards-development)
  - [4.4 Given a scenario, apply the appropriate type of visualization](#44-given-a-scenario-apply-the-appropriate-type-of-visualization)
  - [4.5 Compare and contrast types of reports](#45-compare-and-contrast-types-of-reports)
- [Domain 5.0 Data Governance, Quality, and Controls](#domain-50-data-governance-quality-and-controls)
  - [5.1 Summarize important data governance concepts](#51-summarize-important-data-governance-concepts)
  - [5.2 Given a scenario, apply data quality control concepts](#52-given-a-scenario-apply-data-quality-control-concepts)
  - [5.3 Explain master data management (MDM) concepts](#53-explain-master-data-management-mdm-concepts)
- [CompTIA Data+ (DA0-001) Acronym List](#comptia-data-da0-001-acronym-list)
- [CompTIA Data+ Proposed Hardware and Software List](#comptia-data-proposed-hardware-and-software-list)
  - [Hardware](#hardware)
  - [Software](#software)
- [Reference](#reference)

## Domain 1.0 Data Concepts and Environments

### 1.1 Identitfy basic concepts of data schemas and dimensions

- **Databases**
  - Relational
  - Non-relational
- **Data mart/data warehousing/data lake**(*data warehouse* is a technology that is dedicated to the storage of company data from a wide range of sources for reporting and decision-making purposes (DW-Extract, Transform, Load, DL-Extract, Load, Transform),  (*data mart* is a subset of the data warehouse that is dedicated to a specific department or group), (*data lake* is a technology for storing large amounts of structured and unstructured types of information in their original format.*data lake* isn't acknowledged as the single source of truth like a data warehouse.)(**Data Lakehouse** is a data management system that combines the best of both data warehousing and data lakes)
  - Online transactional processing(*OLTP*) (A class of software that allows large numbers of database transactions in real time, typically over the internet)
  - Online analytical processing(*OLAP*)(A class of software that allows complex analysis to be conducted on large databases without negatively affecting transactional systems)
- **Schema concepts**
  - *Snowflake*(The relational model of fact and dimension tables that looks like a snowflake, in which dimension tables are joined to a single fact table and also other dimension tables)
  - *Star*(The relational model of fact and dimension tables that looks like a star, in which all dimension tables are joined to a single fact table)
- **Slowly changing dimensions**
  - Keep current information
  - Keep historical and current information

### 1.2 Compare and contrast different data types

- Data
- Numeric
- Alphanumeric
- Currency
- Text
- **Discrete vs. continuous**(Quantitative data)(Discrete data is a numerical type of data that includes numbers with fixed data values determined by counting. Continuous data includes complex numbers and varying data values that are measured over a specific time interval.)
- Categorical/dimension (**Categorical** data(qualitative))
- Images
- Audio
- Video

### 1.3 Compare and contrast common data structures and file formats

- **Structures**
  - Structured
    - Defined rows/columns
    - Key value pairs
  - Unstructured
    - Undefined fields
    - Machine data
- **Data file formats**
  - Text/Flat file
    - Tab delimited
    - Comma delimited
  - JavaScript Object Notation(JSON)
  - *Extensible Markup Language(XML)*(Unlike HTML, the primary purpose of XML is to transfer data, not display it.)
  - Hypertext Markup Language(HTML)

## Domain 2.0 Data Mining

### 2.1 Explain data acquisition concepts

- **Integration**
  - Extract, transform, load(ETL is the most commonly used method for *data warehouses*)
  - Extract, load, transform(ELT is the most commonly used method for *data lakes*)
  - Delta load(moves only recently changed data from the transactional system to data the data warehouse)
    - Initial load(moves all data from the transactional system to the data warehouse,)
    - Data lakes(Data lakes often use the ELT process)
  - Application programming
  - Interfaces(APIs)
- **Data collection methods**
  - Web scraping
  - Public databases
  - Application programming
  - Interface(API)/web services
  - Survey
  - Sampling(pobieranie próbek)
  - Observation

### 2.2 Identify common reason for cleansing and profiling datasets

- **Duplicate data**
- **Redundant data**(The same data is represented in multiple places.)
- **Missing values**(MCAR-data that is missing completely at random,MAR-data that is missing at random,MNAR-data that is missing not at randoms)
- **Invalid data**
- **Non-parametric data**
- **Data outliers**(Values in the data set that don't seem to be within the norm of all the other data)
- **Specification mismatch**
- **Data type validation**

### 2.3 Given a scenario, execute data manipulation techniques

- **Recoding data**
  - Numeric
  - Categorical
- **Derived variables**(is a data point that is “derived” or created from existing data.)
- **Data merge**(adds columns,the process of combining two or more data sets into a single data set)
- **Data blending**(combine dataset from diffrent resources and creating a single, unique,
dataset for visualization and analysis)
- **Concatenation**(The CONCAT() function adds two or more strings together)
- **Data append**(adds rows,  a process that involves adding new data elements to an existing database)
- **Imputation**(the process of replacing missing data with substituted values)
- **Reduction/aggregation**
- **Transpose**(To reverse the direction of data. Swap row and columns, is where the data in the rows are turned into columns, and the data in the columns is turned into rows)
- **Normalize data**
- **Parsing**(means to break data into parts)/**string manipulation**((or string handling) is the process of changing, parsing, splicing pasting, or analyzing strings. SQL is used for managing data in a relational database)

### 2.4 Explain common techniques for data manipulation and query optimization

- **Data manipulation**
  - Filtering(Filters allow you to show or hide information on your sheet 
based on selected criteria.)
  - Sorting(Sorting lets you organize all or part of your data in ascending or 
descending order.)
  - Date functions
  - Logical functions
  - Aggregate functions
  - System functions
- **Query optimization**
  - Parametrization(prevent from SQL injection. A parameter takes a 
value only when the query is executed, allowing the query to be 
reused with different values and purposes)
  - Indexing
  - Temporary table in the query set
  - Subset of records
  - Execution plan

## Domain 3.0 Data Analysis

### 3.1 Given a scenario, apply the appropriate descriptive statistical methods

Descriptive Statistics describes our data by summarizing it and providing us with visible features

- **Measures of central tendency**
  - *Mean*(The average of a set of numbers, calculated by adding all the values and then dividing that sum by the total number of values)
  - *Median*(The middle number within a group of sorted numbers)
  - *Mode*(most common value in the dataset)
- **Measures of dispersion**
  - Range( R = Hs – Ls)
    - Max
    - Min
  - Distribution(*normal distribution* A bell-shaped curve that depicts the data distribution for a data set)
  - *Variance*(the average of the squared differences of each value in the dataset from the mean)(symbol σ² used)
  - *Standard deviation*(A value that shows how dispersed the data is in relationship to the mean of all of the data)
- *Frequencies/percentages*
- *Percent change*(Normalizes data to be a proportion of 100,subtracting the newest value from the last value, dividing that number by the last value, and then multiplying by 100%)
- *Percent difference*
- *Confidence intervals*(The calculation of values that describes the certainty or uncertainty of an estimate made on the analysis)

### 3.2 Explain the purpose of inferential statistical methods

Inferential Statistics draws conclusions from our data by making generalizations and predictions

- t-tests(A test used to *compare two groups* when determining whether there is a significant difference between the means of both groups)
- Z-score(A value that shows how many *standard deviations a data point is from the mean*)
- P-values(A value in statistical tests that tells the probability that an observed difference *occurred by chance*)
- Chi-squared(compares the size of the *difference between the expected result and the actual result*,two types:*Goodness of fit* and test of independence)
- Hypothesis testing
  - Type I error(An error that occurs when the *correct hypothesis is rejected* and the *incorrect hypothesis is accepted*; also known as *false positive*)
  - Type II error(An error that occurs when the *incorrect hypothesis is accepted* and the *correct hypothesis is rejected*; also known as *false negative*)
- Simple linear regression(analysis will typically involve an x and y scatter plot with a line)
- **Correlation**(Strength of relationship between two variables)(positive correlation(r>0),negative correlation(r<0),no correlation(r=0))

### 3.3 Summarize types of analysis and key analysis techniques

- Process to determine type of analysis
  - Review/refine business questions
  - Determine data needs and sources to perform analysis
  - Scoping/gap analysis(difference between the desired state and current state)
- Type of analysis(defined by measuring a trend on historical data to predict a future outcome)
  - Trend analysis(Forecasting future values)
    - Comparison of data over time
  - Performance analysis
    - Tracking measurements against defined goals
    - Basic projections to achieve goals
  - Exploratory data analysis(EDA)
    - Use of descriptive statistics to determine observations
  - Link analysis(A network, a node, and a link)
    - Connections of data points or pathway

### 3.4 Identify common data analytics tools

The intent of this objectives is NOT to test specific vendor feature sets nor the purposes of the tools.

- Structured Query Language(SQL)
- Python
- Microsoft Excel
- R
- Rapid mining
- IBM Cognos
- IBM SPSS Modeler
- IBM SPSS
- SAS
- Tableau
- Power BI
- Qlik
- MicroStrategy
- BusinessObjects
- Apex
- Dataroma
- Domo(is a cloud-based platform designed to provide direct, 
simplified, real-time access to business data for decision-makers across the company with minimal IT involvement. It specializes in business intelligence tools and data visualization.)
- AWS QuickSight
- Stata
- Minitab

## Domain 4.0 Visualization

### 4.1 Given a scenario, translate business requirements to form a report

- **Data content**
- **Filtering**(limits report content to relevant data)
- **Views**
- **Date range**(custom report that allows you either to 
select a month to include in the report or to choose specific 
start and end dates for the data included in the report.)
- **Frequency**
- **Audience for report**(Concerning dashboard best practices in design, your audience is one of the *most important principles you have to take into account*)
  - *Distribution list*(The people who are in your audience, who will receive a report or dashboard)

### 4.2 Given a scenario, use appropriate design components for reports and dashboards

- **Report cover page**
  - Instructions
  - Summary
    - Observation and Insights
- **Design elements**
  - Color schemes
  - Layout
  - Front size and styles
  - Key chart elements
    - Title(title-30p,heading 1-26p,heading 2-20p,heading 3-16p,Body text-13p)
    - Labels(to identify each axis)
    - Legends(A labeling element that lets you know which color represents which value in a visual)
  - Corporate reporting standards/style guide
    - Branding
    - Color codes
    - Logos/trademarks
    - Watermark(Information displayed as an overlay on a report indicating content is *confidential or should not be printed or distributed*)
- **Documentation elements**
  - Version number
  - Reference data sources
  - Reference dates
    - Report run date
    - Data refresh date
  - Frequently asked questions(FAQs)(answers frequently asked question)
  - Appendix(include supportive details)

### 4.3 Given a scenario, use appropriate methods for dashboards development

- **Dashboard considerations**
  - *Data source and attributes*
    - Field definitions
    - Dimensions(attributes used to divide data into categories or segments)
    - Measures(quantitive data that will be explored by dashboard users)
  - *Continuous/live data feed vs. static data*
  - *Consumer types*
    - C-level executives
    - Management
    - External vendor/stakeholders
    - General public
    - Technical experts
- **Development process**
  - *Mockup/wireframe*( Mockup simply means to draw out a potential layout. This can happen on a piece of paper or inside a software, like Snagit, where you can draw different screens and add objects. Wireframing is creating mockups of multiple screens that are likely connected. You can do this with paper and pen, or you can use tools like Figma that allow you to draw the screens and add interactions.)
    - Layout/presentation
    - Flow/navigation
    - Data store planning
  - Approval granted
  - Develop dashboard
  - Deploy to production
- **Delivery considerations**
  - Subscriptions
  - Schedule delivery
  - Interactive(drill down/roll up)
    - Saved searches
    - Filtering
  - Static
  - Web interface
  - Dashboard optimization
  - Access permisssions

### 4.4 Given a scenario, apply the appropriate type of visualization

- **Line chart**(compare the values of two variables using two axes)
- **Pie chart**(A circle broken into slices to represent percentages of information)
- **Bubble chart**(This visual will allow you to add a third variable or dimension of the data as represented by the size of the dot (or bubble).)
- **Scatter plot**(have both an x-axis and y-axis.This visual will allow you to add a third variable or dimension of the data as represented by the size of the dot (or bubble).)
- **Bar chart**(Show how data fits into categories,a bar chart reads horizontally, while a column chart reads vertically)
- **Histogram**
- **Waterfall**(A waterfall visualization shows how an initial value is increased and decreased by a series of intermediate values, leading to a final cumulative value shown in the far right column)
- **Heat map**(is any visual that uses color to draw attention to a “hot” spot)
- **Geographic map**(The purpose of the Geographic map is to support the analysis f geospatial data through the use of interactive voisualization. Map visualization is used to analyze and display geographically related data and present it in the form of maps)
- **Tree map**(designed for nested datasets)
- **Stacked chart**(A chart that breaks the bar or column into separate portions, each representing an additional data point)
- **Infographic**(tells a visual story.Is any combination of visuals, artwork, photos, and language that tells the story of your data in a compelling and graphically appealing way.)
- **Word cloud**(is a visual representation of the words used in a particular body of texts)

### 4.5 Compare and contrast types of reports

- **Static vs. dynamic reports**(*Static reporting* requires pulling up various reports from different sources and analyzing insights from a longer time period in order to provide a snapshot of data while 
*Dynamic reports* provide deep insights, and allow users to interact with the data rather than just view it.)
  - *Point-in-time*(are frequently used for presentations and meetings, and they are typically scheduled to be run on a set schedule.)
  - *Real time*(is particularly useful for dashboarding. When we can refresh our dashboards and see what’s happening in “real time,”.)
- **Ad-hoc/one-time report**(Ad hoc reports are generated as needed for a one-timeuse, in a visual format relevant to the audience.)
- **Self-service/on-demand**(or on-demand report, is one that is run directly by the consumer. When business users can leverage dashboards, or run their own reports from the systems the organization has purchased, they are performing what we call self-service)
- **Recuring reports**(Generated on a planed schedule)
  - *Compliance report*(e.g., financial, health, and safety. Presenting information to auditors that show that your company is adhering to all the 
requirements set by the government and regulatory agency)
  - *Risk and regulatory reports*
  - *Operational reports*( is an effective, results-driven means of 
tracking, measuring, and analyzing a business’s regular deliverables and metrics, usually on a daily, weekly, and sometimes monthly basis with the help of modern and professional BI reporting tools.)
- **Tactical /research report**(Tactical report provide information to support short-term decisions, Research report provide information to support significant,strategic decisions)

## Domain 5.0 Data Governance, Quality, and Controls

### 5.1 Summarize important data governance concepts

- **Access requirements**
  - *Role-based*(Role-based access control (RBAC), also known as role-based security, is an access control method that assigns permissions to end-users based on their role within your organization)
  - *User group-based*
  - *Data use agreements*
  - *Release approvals*
- **Security requirements**
  - *Data encryption*(is the process of using algorithms that will “scramble” data from its original plaintext into another form, known as cyphertext, so that it can’t be read. This process ensures that sensitive data is not exposed while in transit.)
  - *Data transmission*(is the process of transferring (sending or receiving) data)
  - *De-identify data/data masking*(masking The act of hiding the original value of data by showing something else in its place; *also known as anonymization*)
- **Storage environment requirements**
  - *Shared drive vs. cloud based vs. local storage*
- **Use requirements**
  - *Acceptable use policy*
  - *Data processing*
  - *Data deletion*
  - *Data retention*
- **Entity relationship requirements**
  - *Record link restrictions*(There are some instances in which our data systems cannot ever be related to each other due to organizational policies. These cases are known as record link restrictions, and they are set due to regulations intended to protect data sets with sensitive information.)
  - *Data constraints*
  - *Cardinality*
- **Data classification**
  - *Personally identifiable information*(PII)(Traceable to a specific person)
  - *Personal health information*(PHI)(Covered by HIPAA)
  - *Payment card industry*(PCI)(Covered by PCI DSS)
- **Jurisdiction requirements**
  - *Impact of industry and governmental regulations*
- **Data breach reporting**(occurs when information is read, modified, or deleted without authorization. "Read" in this sense can mean either seen by a person or transferred to a network or storage media.)
  - *Escalate to appropriate authority*

### 5.2 Given a scenario, apply data quality control concepts

- **Circumstances to check for quality**
  - *Data acquisition/data source*
  - *Data transformation/intrahops*
    - *Pass through*
    - *Conversion*
  - *Data manipulation*
  - *Final product* (report/dashboard, etc.)
- **Automated validation**
  - *Data field to data type validation*
  - *Number of data points*
- **Data quality dimensions**
  - *Data consistency*(dimension of data quality ensures that data stored in multiple locations is the sames)
  - *Data accuracy*(is the degree to which data correctly reflects the realworld object OR an event being described.)
  - *Data completeness*
  - *Data integrity*
  - *Data attribute limitations*
- **Data quality rule and metrics**
  - *Conformity*
  - *Non-conformity*
  - *Rows passed*
  - *Rows failed*
- **Methods to validate quality**
  - *Cross-validation*
  - *Sample/spot check*
  - *Reasonable expectations*
  - *Data proiling*
  - *Data audits*

### 5.3 Explain master data management (MDM) concepts

- **Processes**
  - Consolidation of multiple data fields
  - Standardization of data field names
  - *Data dictionary*( is an authoritative document that defines all master data and key metrics at an organization.)
- **Circumstances for MDM**
  - Mergers and acquisitions
  - Compliance with policies and regulations
  - Streamline data access

## CompTIA Data+ (DA0-001) Acronym List

> The following is a list of acronyms that appear on the CompTIA Data+ exam. Candidates are encourage to review the complate list and attain a working knowledge of all listed acronyms as a part of a comprehensive exam preparation program.

|ACRONYM|DEFINITION|
|---|---|
|ANOVA|Analysis of Variance|
|API|Application Programming Interface|
|AWS|Amazon Web Services|
|BI|Business Intelligence|
|CRM|Customer Relationship Management|
|CSV|Comma-separated Values|
|ELT|Extract, Load, Transform|
|ETL|Extract, Transform, Load|
|FAQs|Frequently Asked Question|
|GDPR|General Data Protection Regulation|
|HTML|Hypertext Markup Language|
|JSON|JavaScript Object Notation|
|KPI|Key Performance Indicator|
|MDM|Master Data Management|
|NoSQL| Not Only Structured Query Language|
|OLAP|Online Analytical Processing|
|OLTP|Online Transaction Processing|
|P&L|Profit and Loss|
|PCI|Payment Card Industry|
|PDF|Portable Document Format|
|PHI|Personal Health Information|
|PII|Personally Identifiable Information|
|RDBMS|Relational Database Management System|
|SDLC|Software Development Life Cycle|
|SQL|Structured Query Language|
|XML|Extensible Markup Language|

## CompTIA Data+ Proposed Hardware and Software List

> CompTIA has included this sample list of hardware and software to assist candidates as they prepare for the Data+ exam. This list may also be helpful for training companies that wish to create a lab component for their training offering. The bulleted lists below each each topic are samples and not exhaustive.

### Hardware

- Desktop/laptop
  - High processing power for large volume analyses
  - Lower processing power for smaller volume analyses
- Internet access
- Cloud environment

### Software

- SQL environment to run scripts(SQL Lite, Management Studio, etc.)
- Eclipse
- Anaconda
- R Studio
- Database modeling tool
- Microsoft Office Suite
- Visualization tools(Tableau, Power BI, etc.)
- Reporting tools
- Sample datasets(Kaggle)

## Reference

[CompTIA](https://comptia.org)

---
