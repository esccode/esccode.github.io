---
title: CompTIA Data+ Domain
categories:
- data
excerpt: |
## CompTIA Data+ Domain excerpt
feature_text: |  
  ## @esccode
  “It is better to be hated for what you are than to be loved for what you are not.” ― Andre Gide, Autumn Leaves
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
aside: true
---

- [Domain 1.0 Data Concepts and Environments](#domain-10-data-concepts-and-environments)
  - [1.1 Identitfy basic concepts of data schemas and dimensions](#11-identitfy-basic-concepts-of-data-schemas-and-dimensions)
  - [1.2 Compare and contrast different data types](#12-compare-and-contrast-different-data-types)
  - [1.3 Compare and contrast common data structures and file formats](#13-compare-and-contrast-common-data-structures-and-file-formats)
- [Domain 2.0 Data Mining](#domain-20-data-mining)
  - [2.1 Explain data acquisition concepts](#21-explain-data-acquisition-concepts)
  - [2.2 Identify common reason for cleansing and profiling datasets](#22-identify-common-reason-for-cleansing-and-profiling-datasets)
  - [2.3 Given a scenario, execute data manipulation techniques](#23-given-a-scenario-execute-data-manipulation-techniques)
  - [2.4 Explain common techniques for data manipulation and query optimization](#24-explain-common-techniques-for-data-manipulation-and-query-optimization)
- [Domain 3.0 Data Analysis](#domain-30-data-analysis)
  - [3.1 Given a scenario, apply the appropriate descriptive statistical methods](#31-given-a-scenario-apply-the-appropriate-descriptive-statistical-methods)
  - [3.2 Explain the purpose of inferential statistical methods](#32-explain-the-purpose-of-inferential-statistical-methods)
  - [3.3 Summarize types of analysis and key analysis techniques](#33-summarize-types-of-analysis-and-key-analysis-techniques)
  - [3.4 Identify common data analytics tools](#34-identify-common-data-analytics-tools)
- [Domain 4.0 Visualization](#domain-40-visualization)
  - [4.1 Given a scenario, translate business requirements to form a report](#41-given-a-scenario-translate-business-requirements-to-form-a-report)
  - [4.2 Given a scenario, use appropriate design components for reports and dashboards](#42-given-a-scenario-use-appropriate-design-components-for-reports-and-dashboards)
  - [4.3 Given a scenario, use appropriate methods for dashboards development](#43-given-a-scenario-use-appropriate-methods-for-dashboards-development)
  - [4.4 Given a scenario, apply the appropriate type of visualization](#44-given-a-scenario-apply-the-appropriate-type-of-visualization)
  - [4.5 Compare and contrast types of reports](#45-compare-and-contrast-types-of-reports)
- [Domain 5.0 Data Governance, Quality, and Controls](#domain-50-data-governance-quality-and-controls)
  - [5.1 Summarize important data governance concepts](#51-summarize-important-data-governance-concepts)
  - [5.2 Given a scenario, apply data quality control concepts](#52-given-a-scenario-apply-data-quality-control-concepts)
  - [5.3 Explain master data management (MDM) concepts](#53-explain-master-data-management-mdm-concepts)
- [CompTIA Data+ (DA0-001) Acronym List](#comptia-data-da0-001-acronym-list)
- [CompTIA Data+ Proposed Hardware and Software List](#comptia-data-proposed-hardware-and-software-list)
  - [Hardware](#hardware)
  - [Software](#software)
- [Reference](#reference)

## Domain 1.0 Data Concepts and Environments

### 1.1 Identitfy basic concepts of data schemas and dimensions

- **Databases**
  - *Relational*
  - *Non-relational*
- **Data mart/data warehousing/data lake**(*data warehouse* is a technology that is dedicated to the storage of company data from a wide range of sources for reporting and decision-making purposes (DW-Extract, Transform, Load, DL-Extract, Load, Transform),  (*data mart* is a subset of the data warehouse that is dedicated to a specific department or group), (*data lake* is a technology for storing large amounts of structured and unstructured types of information in their original format. *data lake* isn't acknowledged as the single source of truth like a data warehouse.)(**Data Lakehouse** is a data management system that combines the best of both data warehousing and data lakes)
  - *Online transactional processing*(*OLTP*) (A class of software that allows large numbers of database transactions in real time, typically over the internet)
  - *Online analytical processing*(*OLAP*)(A class of software that allows complex analysis to be conducted on large databases without negatively affecting transactional systems)
- **Schema concepts**
  - *Snowflake*(The relational model of fact and dimension tables that looks like a snowflake, in which dimension tables are joined to a single fact table and also other dimension tables)
  - *Star*(The relational model of fact and dimension tables that looks like a star, in which all dimension tables are joined to a single fact table)
- **Slowly changing dimensions**(Dimension data does not change very often, but it is used alongside fact data that changes regularly.There are three primary types of slowly changing dimensions:
**Slowly Changing Dimension Type 1** overwrites the existing value in a field of data with a new value and does not retain the history of that field.
**Slowly Changing Dimension Type 2** adds a new row for the new value and also maintains the historical record.
**Slowly Changing Dimension Type 3** adds a new column for the current value but also retains the original column with the original value.)
  - *Keep current information*
  - *Keep historical and current information*

### 1.2 Compare and contrast different data types

- **Date**
- **Numeric**
- **Alphanumeric**
- **Currency**
- **Text**
- **Discrete vs. continuous**(Quantitative data)(Discrete data is a numerical type of data that includes numbers with fixed data values determined by counting. Continuous data includes complex numbers and varying data values that are measured over a specific time interval.)
- **Categorical/dimension** (Categorical data(qualitative))
- **Images**
- **Audio**
- **Video**

### 1.3 Compare and contrast common data structures and file formats

- **Structures**
  - *Structured*(Exel file,SQL databeses)
    - *Defined rows/columns*
    - *Key value pairs*
  - *Unstructured*(No-SQL databases,Audio file)
    - *Undefined fields*
    - *Machine data*
- **Data file formats**
  - *Text/Flat file*
    - *Tab delimited*
    - *Comma delimited*
  - *JavaScript Object Notation(JSON)*(It is primarily used for transmitting data between a web application and a server)
  - *Extensible Markup Language(XML)*(Unlike HTML, the primary purpose of XML is to transfer data, not display it.)
  - *Hypertext Markup Language(HTML)*

## Domain 2.0 Data Mining

### 2.1 Explain data acquisition concepts

- **Integration**
  - *Extract, transform, load*(ETL is the most commonly used method for *data warehouses*)
  - *Extract, load, transform*(ELT is the most commonly used method for *data lakes*)
  - *Delta load*(moves only recently changed data from the transactional system to data the data warehouse)
    - *Initial load*(moves all data from the transactional system to the data warehouse,)
    - *Data lakes*(Data lakes often use the ELT process)
  - *Application programming*
  - *Interfaces(APIs)*
- **Data collection methods**
  - *Web scraping*(Web scraping is the act of pulling information from a website)
  - *Public databases*
  - *Application programming*
  - *Interface(API)/web services*
  - *Survey*( survey is defined as the act of examining a process or questioning a selected sample of individuals to obtain data about a service, product, or process. Data collection surveys collect information from a targeted group of people about their opinions, behavior, or knowledge.)
  - *Sampling(pobieranie próbek)*
  - *Observation*

### 2.2 Identify common reason for cleansing and profiling datasets

- **Duplicate data**
- **Redundant data**(The same data is represented in multiple places.)
- **Missing values**(MCAR-data that is missing completely at random,MAR-data that is missing at random,MNAR-data that is missing not at randoms)
- **Invalid data**
- **Non-parametric data**(Data that is not within the rules of normal distribution, with values that frequently deviate from the mean)
- **Data outliers**(Values in the data set that don't seem to be within the norm of all the other data)
- **Specification mismatch**
- **Data type validation**

### 2.3 Given a scenario, execute data manipulation techniques

- **Recoding data**
  - *Numeric*
  - *Categorical*
- **Derived variables**(is a data point that is “derived” or created from existing data.)
- **Data merge**(adds columns,the process of combining two or more data sets into a single data set)
- **Data blending**(combine dataset from diffrent resources and creating a single, unique,
dataset for visualization and analysis)
- **Concatenation**(The CONCAT() function adds two or more strings together)
- **Data append**(adds rows,  a process that involves adding new data elements to an existing database)
- **Imputation**(the process of replacing missing data with substituted values)
- **Reduction**(Reducing the volume of data)/**aggregation**
- **Transpose**(To reverse the direction of data. Swap row and columns, is where the data in the rows are turned into columns, and the data in the columns is turned into rows)
- **Normalize data**(Data normalization is the process of structuring your relational customer database, following a series of normal forms. This improves the accuracy and integrity of your data while ensuring that your database is easier to navigate)
- **Parsing**(means to break data into parts)/**string manipulation**((or string handling) is the process of changing, parsing, splicing pasting, or analyzing strings. SQL is used for managing data in a relational database)

### 2.4 Explain common techniques for data manipulation and query optimization

- **Data manipulation**
  - *Filtering*(Filters allow you to show or hide information on your sheet based on selected criteria.)
  - *Sorting*(Sorting lets you organize all or part of your data in ascending or descending order.)
  - *Date functions*(now(), today(), datediff(), networkdays(), weekday(), weeknum(),month())
  - *Logical functions*(IF, ISNULL, AND, and OR)
  - *Aggregate functions*(sum(),count(),distinct count(),average(),max(),min())
  - *System functions*(use system functions in the headers and footers of your reports such as page numbers, refresh dates, report names, and more)
- **Query optimization**
  - *Parametrization*(prevent from SQL injection. A parameter takes a value only when the query is executed, allowing the query to be reused with different values and purposes)
  - *Indexing*(is a field property setting that tells the database that a field needs to be indexed.When fields are indexed, it makes data processing faster and ultimately speeds up the performance of the query.ndexing makes columns faster to query by creating pointers to where data is stored within a database.)
  - *Temporary table in the query set*(is a table that is stored on the database server until a user disconnects from the server. Similar to a permanent table, temporary tables provide records, but they are for temporary use only. These temporary tables can improve processing speeds for queries simply because they likely contain a smaller number of records than the permanent table.)
  - *Subset of records*(also commonly referred to as a nested query, is a query that is nested inside another query statement.)
  - *Execution plan*(is the order of steps in which a query is processed. It is a visual representation that provides details about how the query executes.Two types:Estimated execution plan and actual execution plan)

## Domain 3.0 Data Analysis

### 3.1 Given a scenario, apply the appropriate descriptive statistical methods

Descriptive Statistics describes our data by summarizing it and providing us with visible features

- **Measures of central tendency**
  - *Mean*(The average of a set of numbers, calculated by adding all the values and then dividing that sum by the total number of values. In exel this function is called average.)
  - *Median*(The middle number within a group of sorted numbers)
  - *Mode*(Most common value in the dataset.A single mode is one single number that appears most often in the data set.A multiple mode exists when two or more numbers appear an equal number of times. In Excel, you can use the calculations =MODE.SNGL or =MODE.MULT to return the mode on a group of numbers.)
- **Measures of dispersion**
  - *Range*( R = Hs – Ls)(is the interval between the highest and the lowest score. Range is a measure of variability or scatteredness of the varieties or observations among themselves and does not give 89 an idea about the spread of the observations around some central value. We subtract the minimum number (55) from the maximum number (99) to calculate the range.)
    - *Max*(r maximum, is the largest number in the data set.)
    - *Min*(which stands for minimum, is the smallest number in the data set.)
  - *Distribution*(*normal distribution* A bell-shaped curve that depicts the data distribution for a data set)
  - *Variance*(the average of the squared differences of each value in the dataset from the mean)(symbol σ² used.)
  - *Standard deviation*(A value that shows how dispersed the data is in relationship to the mean of all of the data. A low standard deviation indicates that the values are closer to the mean, whereas a high standard deviation tells you the values are more spread out, and thus further from the mean. Standard deviation is a statistical measurement in finance that, when applied to the annual rate of return of an investment, sheds light on that investment’s historical volatility.)
- **Frequencies/percentages**(is the number of times that a data point occurs within a data set. Calculating the frequency of any data point is as simple as grouping the data and counting it up. You can do this in several ways. One method is to use the SUBTOTAL command in Excel.)
- **Percent change**(Normalizes data to be a proportion of 100,subtracting the newest value from the last value, dividing that number by the last value, and then multiplying by 100%)
- **Percent difference**(is a calculation performed by determining the absolute value of the difference between two numbers, dividing by the average of the two values, and then multiplying by 100%. This displays the product as a percentage)
- **Confidence intervals**(Estimate how close sa sample mean is to the actual population mean. The calculation of values that describes the certainty or uncertainty of an estimate made on the analysis. CONFIDENCE Function in Excel)

### 3.2 Explain the purpose of inferential statistical methods

Inferential Statistics draws conclusions from our data by making generalizations and predictions

- **t-tests**(A test used to compare two groups when determining whether there is a significant difference between the means of both groups. Is used to produce a 'p' value when you conducting hypothesis testing. A t-test is used as a hypothesis testing tool, which allows testing of an assumption applicable to a population)
- **Z-score**(A value that shows how many standard deviations a data point is from the mean.In Excel, we can use the STANDARDIZE () function to calculate z-scores. A Z-score is a numerical measurement that describes a value’s relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean)
- **P-values**(Probability that test results are not statistically significant. A high p-value indicates that the outcome is likely not repeatable and thus not significant, whereas a low p-value tells us the event is likely repeatable, meaning it did not occur due to chance and is thus statistically significant.)
- **Chi-squared**(Determines whether categorical variables are associated. Compares the size of the difference between the expected result and the actual result,two types:Goodness of fit and test of independence. Goodness of fit is used for the distribution of the data and the test of independence is used for the relationship between the variables.)
- **Hypothesis testing**(An alternative hypothesis assumes that a relationship exists between two variables, while a null hypothesis assumes that a relationship does not exist between two variables.)
  - *Type I error*(An error that occurs when the correct hypothesis is rejected and the incorrect hypothesis is accepted; also known as false positive)
  - *Type II error*(An error that occurs when the incorrect hypothesis is accepted and the correct hypothesis is rejected; also known as false negative)
- **Simple linear regression**(analysis will typically involve an x and y scatter plot with a line. Simple linear regression is used to study the relationship between one independent variable and one dependent variable, and it is simple because there are only two variables involved.)
- **Correlation**(Strength of relationship between two variables)(positive correlation(r>0),negative correlation(r<0),no correlation(r=0).Correlation is the statistical association between two (or more) equal variables that tells us if one variable changes, the other(s) will too.)

### 3.3 Summarize types of analysis and key analysis techniques

- **Process to determine type of analysis**
  - *Review/refine business questions*(ask clarifying questions to identify any gaps in tha data and resources you need)
  - *Determine data needs and sources to perform analysis*
  - *Scoping/gap analysis*(is the study of a present state, desired state, and the gaps between the two)
- **Type of analysis**(defined by measuring a trend on historical data to predict a future outcome)
  - *Trend analysis*(Forecasting future values)
    - *Comparison of data over time*
  - *Performance analysis*(is analysis that is done to measure the performance of a particular product, outcome, or scenario against a defined objective. Performance analysis uses qualitative and quantitative data.)
    - *Tracking measurements against defined goals*
    - *Basic projections to achieve goals*
  - *Exploratory data analysis(EDA)*(is something you should be prepared to perform on literally every data set you encounter. The point of exploratory analysis is to determine the main characteristic of the data set. All data should be explored through exploratory analysis before it is analyzed any further)
    - *Use of descriptive statistics to determine observations*
  - *Link analysis*(is used to determine how a single data point links to other data points, focusing on the relationships and connections in a database.Link analysis has three main components: a network, a node, and a link. )
    - *Connections of data points or pathway*

### 3.4 Identify common data analytics tools

The intent of this objectives is NOT to test specific vendor feature sets nor the purposes of the tools.

- **Structured Query Language(SQL)**(Data definition language(DDL), Data manipulation Language(DML). CREATE-sql ddl command is used to create a new table,ALTER command is used to change the structure of a table you already created. DROP-command that deletes entire tables and databases. SELECT command is to retrieves information from databases. INSERT command that adds new record to a databases table. UPDATE is used to modified info in database. DELETE is to delete rows in databases.)
- **Python**
- **Microsoft Excel**
- **R**(free, open-source language. Implements most modern machine learning methods. Simplifies data analysis using the tidyverse.)
- **Rapid mining**(machine learning tool)
- **IBM Cognos**(Analytics suite)
- **IBM SPSS Modeler**
- **IBM SPSS**(a popular statistical analysis tool(statistical package), used to machine learning)
- **SAS**
- **Tableau**(for reporting/visualization )
- **Power BI**(analytics suite - for reporting/visualization)
- **Qlik**((Analytics suite). SaaS platform. for visualization)
- **MicroStrategy**(Analytics suite)
- **BusinessObjects**(SAP enterprise reporting tool)
- **Apex**
- **Dataroma**(Analytics suite for sales and marketing)
- **Domo**(is a cloud-based platform designed to provide direct, simplified, real-time access to business data for decision-makers across the company with minimal IT involvement. It specializes in business intelligence tools and data visualization. Is software as a service(SaaS analytics platform))
- **AWS QuickSight**(Analytics suite, pay as you go basis)
- **Stata**(less widely used than SPSS and SAS. Statistical analysis)
- **Minitab**(statistics package. Shares most of the same features like SPSS,SAS,STATA not widely used  today. Statistical analysis)

## Domain 4.0 Visualization

### 4.1 Given a scenario, translate business requirements to form a report

- **Data content**
- **Filtering**(limits report content to relevant data)
- **Views**
- **Date range**(custom report that allows you either to 
select a month to include in the report or to choose specific 
start and end dates for the data included in the report.)
- **Frequency**
- **Audience for report**(Concerning dashboard best practices in design, your audience is one of the *most important principles you have to take into account*)
  - *Distribution list*(The people who are in your audience, who will receive a report or dashboard)

### 4.2 Given a scenario, use appropriate design components for reports and dashboards

- **Report cover page**
  - *Instructions*
  - *Summary*
    - *Observation and Insights*
- **Design elements**
  - *Color schemes*
  - *Layout*
  - *Front size and styles*
  - *Key chart elements*
    - *Title*(title-30p,heading 1-26p,heading 2-20p,heading 3-16p,Body text-13p)
    - *Labels*(to identify each axis)
    - *Legends*(A labeling element that lets you know which color represents which value in a visual)
  - *Corporate reporting standards/style guide*
    - *Branding*
    - *Color codes*
    - *Logos/trademarks*
    - *Watermark*(Information displayed as an overlay on a report indicating content is *confidential or should not be printed or distributed*)
- **Documentation elements**
  - *Version number*
  - *Reference data sources*
  - *Reference dates*
    - *Report run date*
    - *Data refresh date*
  - *Frequently asked questions*(FAQs)(answers frequently asked question)
  - *Appendix*(include supportive details)

### 4.3 Given a scenario, use appropriate methods for dashboards development

- **Dashboard considerations**
  - *Data source and attributes*
    - *Field definitions*
    - *Dimensions*(attributes used to divide data into categories or segments)
    - *Measures*(quantitive data that will be explored by dashboard users)
  - *Continuous/live data feed vs. static data*
  - *Consumer types*
    - *C-level executives*
    - *Management*
    - *External vendor/stakeholders*
    - *General public*
    - *Technical experts*
- **Development process**
  - *Mockup(simply means to draw out a potential layout.)/wireframe*( is creating mockups of multiple screens that are likely connected.)
    - *Layout/presentation*
    - *Flow/navigation*
    - *Data store planning*
  - *Approval granted*
  - *Develop dashboard*
  - *Deploy to production*
- **Delivery considerations**
  - *Subscriptions*
  - *Schedule delivery*
  - *Interactive(drill down/roll up)*
    - *Saved searches*
    - *Filtering*
  - *Static*
  - *Web interface*
  - *Dashboard optimization*
  - *Access permisssions*

### 4.4 Given a scenario, apply the appropriate type of visualization

- **Line chart**(consists of either a single horizontal line or a group of multiple lines that represent different data points at different times)
- **Pie chart**(a circle broken into slices to represent percentages of information. The pie chart is easy to understand and thus is commonly used.)
- **Bubble chart**(This visual will allow you to add a third variable or dimension of the data as represented by the size of the dot (or bubble).)
- **Scatter plot**(Stacked charts are useful when you want to add another dimension of data to your visual.)
- **Bar chart**(Show how data fits into categories,a bar chart reads horizontally, while a column chart reads vertically)
- **Histogram**(Histograms use bins to visualize the frequency of your data, and are used quite often in statistics analysis.)
- **Waterfall**(A waterfall visualization shows how an initial value is increased and decreased by a series of intermediate values, leading to a final cumulative value shown in the far right column)
- **Heat map**(is any visual that uses color to draw attention to a “hot” spot)
- **Geographic map**(A dot map is used to draw attention to a particular point on a map, while a filled map is used to highlight the entire boundary of a country, state, city, or county.)
- **Tree map**(Designed for nested datasets. Is a rectangle that shows the proportion of values using smaller rectangles within the larger one. Treemaps also can display a hierarchy when visualizing two data points, with a breakdown of both data points)
- **Stacked chart**(A chart that breaks the bar or column into separate portions, each representing an additional data point)
- **Infographic**(tells a visual story.Is any combination of visuals, artwork, photos, and language that tells the story of your data in a compelling and graphically appealing way.People who work in graphic design might use Adobe Illustrator, while those who are less graphically savvy may use a tool that provides a little more hand-holding, like Canva or another online infographic tool.)
- **Word cloud**(is a visual representation of the words used in a particular body of texts)

### 4.5 Compare and contrast types of reports

- **Static vs. dynamic reports**(*Static reporting* requires pulling up various reports from different sources and analyzing insights from a longer time period in order to provide a snapshot of data while 
*Dynamic reports* provide deep insights, and allow users to interact with the data rather than just view it.)
  - *Point-in-time*(are frequently used for presentations and meetings, and they are typically scheduled to be run on a set schedule.)
  - *Real time*(is particularly useful for dashboarding. When we can refresh our dashboards and see what’s happening in “real time,”.)
- **Ad-hoc/one-time report**(Ad hoc reports are generated as needed for a one-timeuse, in a visual format relevant to the audience.)
- **Self-service/on-demand**(or on-demand report, is one that is run directly by the consumer. When business users can leverage dashboards, or run their own reports from the systems the organization has purchased, they are performing what we call self-service)
- **Recuring reports**(Generated on a planed schedule)
  - *Compliance report*(e.g., financial, health, and safety. Presenting information to auditors that show that your company is adhering to all the 
requirements set by the government and regulatory agency)
  - *Risk and regulatory reports*
  - *Operational reports*( is an effective, results-driven means of 
tracking, measuring, and analyzing a business’s regular deliverables and metrics, usually on a daily, weekly, and sometimes monthly basis with the help of modern and professional BI reporting tools.)
- **Tactical /research report**(Tactical report provide information to support short-term decisions, Research report provide information to support significant,strategic decisions)

## Domain 5.0 Data Governance, Quality, and Controls

### 5.1 Summarize important data governance concepts

- **Access requirements**
  - *Role-based*(Role-based access control (RBAC), also known as role-based security, is an access control method that assigns permissions to end-users based on their role within your organization)
  - *User group-based*
  - *Data use agreements*(two types:The non-disclosure agreement (NDA) and memorandum of understanding (MOU))
  - *Release approvals*(The process will typically occur when you need to work with private information and data that has sensitive and confidential classifications)
- **Security requirements**
  - *Data encryption*(is the process of using algorithms that will “scramble” data from its original plaintext into another form, known as cyphertext, so that it can’t be read. This process ensures that sensitive data is not exposed while in transit.)
  - *Data transmission*(is the process of transferring (sending or receiving) data)
  - *De-identify data/data masking*(masking The act of hiding the original value of data by showing something else in its place; *also known as anonymization*)
- **Storage environment requirements**
  - *Shared drive vs. cloud based vs. local storage*
- **Use requirements**
  - *Acceptable use policy*
  - *Data processing*
  - *Data deletion*
  - *Data retention*
- **Entity relationship requirements**
  - *Record link restrictions*(There are some instances in which our data systems cannot ever be related to each other due to organizational policies. These cases are known as record link restrictions, and they are set due to regulations intended to protect data sets with sensitive information.)
  - *Data constraints*(are integrity rules that limit the types of data that can go into a column or table within a database system)
  - *Cardinality*
- **Data classification**
  - *Personally identifiable information*(PII)(Traceable to a specific person)
  - *Personal health information*(PHI)(Covered by HIPAA)
  - *Payment card industry*(PCI)(Covered by PCI DSS)
- **Jurisdiction requirements**
  - *Impact of industry and governmental regulations*
- **Data breach reporting**(occurs when information is read, modified, or deleted without authorization. "Read" in this sense can mean either seen by a person or transferred to a network or storage media.)
  - *Escalate to appropriate authority*

### 5.2 Given a scenario, apply data quality control concepts

- **Circumstances to check for quality**
  - *Data acquisition/data source*
  - *Data transformation/intrahops*(should always include a process for quality checks)
    - *Pass through*
    - *Conversion*
  - *Data manipulation*
  - *Final product* (report/dashboard, etc.)
- **Automated validation**
  - *Data field to data type validation*
  - *Number of data points*
- **Data quality dimensions**
  - *Data consistency*(dimension of data quality ensures that data stored in multiple locations is the sames)
  - *Data accuracy*(is the degree to which data correctly reflects the realworld object OR an event being described.)
  - *Data completeness*
  - *Data integrity*
  - *Data attribute limitations*
- **Data quality rule and metrics**
  - *Conformity*
  - *Non-conformity*
  - *Rows passed*
  - *Rows failed*
- **Methods to validate quality**
  - *Cross-validation*
  - *Sample/spot check*
  - *Reasonable expectations*
  - *Data profiling*
  - *Data audits*

### 5.3 Explain master data management (MDM) concepts

- **Processes**
  - Consolidation of multiple data fields
  - Standardization of data field names
  - *Data dictionary*( is an authoritative document that defines all master data and key metrics at an organization.)
- **Circumstances for MDM**
  - Mergers and acquisitions
  - Compliance with policies and regulations
  - Streamline data access

## CompTIA Data+ (DA0-001) Acronym List

> The following is a list of acronyms that appear on the CompTIA Data+ exam. Candidates are encourage to review the complate list and attain a working knowledge of all listed acronyms as a part of a comprehensive exam preparation program.

|ACRONYM|DEFINITION|
|---|---|
|ANOVA|Analysis of Variance|
|API|Application Programming Interface|
|AWS|Amazon Web Services|
|BI|Business Intelligence|
|CRM|Customer Relationship Management|
|CSV|Comma-separated Values|
|ELT|Extract, Load, Transform|
|ETL|Extract, Transform, Load|
|FAQs|Frequently Asked Question|
|GDPR|General Data Protection Regulation|
|HTML|Hypertext Markup Language|
|JSON|JavaScript Object Notation|
|KPI|Key Performance Indicator|
|MDM|Master Data Management|
|NoSQL| Not Only Structured Query Language|
|OLAP|Online Analytical Processing|
|OLTP|Online Transaction Processing|
|P&L|Profit and Loss|
|PCI|Payment Card Industry|
|PDF|Portable Document Format|
|PHI|Personal Health Information|
|PII|Personally Identifiable Information|
|RDBMS|Relational Database Management System|
|SDLC|Software Development Life Cycle|
|SQL|Structured Query Language|
|XML|Extensible Markup Language|

## CompTIA Data+ Proposed Hardware and Software List

> CompTIA has included this sample list of hardware and software to assist candidates as they prepare for the Data+ exam. This list may also be helpful for training companies that wish to create a lab component for their training offering. The bulleted lists below each each topic are samples and not exhaustive.

### Hardware

- Desktop/laptop
  - High processing power for large volume analyses
  - Lower processing power for smaller volume analyses
- Internet access
- Cloud environment

### Software

- SQL environment to run scripts(SQL Lite, Management Studio, etc.)
- Eclipse
- Anaconda
- R Studio
- Database modeling tool
- Microsoft Office Suite
- Visualization tools(Tableau, Power BI, etc.)
- Reporting tools
- Sample datasets(Kaggle)

## Reference

[CompTIA](https://comptia.org)

---
